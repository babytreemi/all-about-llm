## LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
## Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
## P-Tuning: GPT Understands, Too
## Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning
## AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning
## LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention
## IA3: Infused Adapter by Inhibiting and Amplifying Inner Activations